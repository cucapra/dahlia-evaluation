# Dahlia Evaluation

This repository contains the evaluation materials for the PLDI 2020 paper
"[Predictable Accelerator Design with Time-Sensitive Affine Types][dahlia-paper]"
using the [Dahlia programming language][dahlia].

There are three components to the evaluation:

- [The Dahlia Compiler][dahlia]: A compiler from Dahlia to Vivado HLS C.
- [Polyphemus Server][polyphemus]: A client–server system for orchestrating large-scale FPGA experiments.
- Benchmarks (this repository).

[dahlia]: https://github.com/cucapra/dahlia
[dahlia-paper]: https://rachitnigam.com/files/pubs/dahlia.pdf
[polyphemus]: https://github.com/cucapra/polyphemus/

### Prerequisites

If you're using the virtual machine image (see below), you just need the hypervisor.
Otherwise, to set up the evaluation outside of the VM, start by cloning this repository.
You will need these prerequisites:

1. Get Python 3 if you don't already have it
2. Install [GNU `parallel`][parallel]
3. Install [Jupyter][] with `pip3 install jupyter`
4. Install other Python dependencies with `pip3 install -r requirements.txt` (in this repository)
5. Install the local benchmarking helpers with `cd benchmarking-helpers && pip install -e .`
6. Run the sanity checking script `./_scripts/sanity-check.sh` to make sure the
   all the tools are configured correctly.

[jupyter]: https://jupyter.org

## Artifact Evaluation

### Getting Started Guide

- Download the artifact VM Image. **TODO: Add link to artifact**
- (*Optional, but recommended*) Enable [multiple physical cores][multiple-cores] for the Virtual Machine. Parts of the evaluation can be completed faster with multiple core support enabled within the VM.
- Boot the image in your favorite hypervisor (we tested the image using [VirtualBox][]).
- Open a terminal and type `cd Desktop/dahlia-evaluation`.
- In the current shell, type `export BUILDBOT=http://cerberus.cs.cornell.edu`.
- Run `./_scripts/sanity-check.sh`. The script should report no errors.
- Run `ESTIMATE=100 ./_scripts/run-dahlia-accepts.sh`. This script will generate configurations for the benchmarks reported in the paper and run the dahlia compiler on 100 of them. It will also generate an estimate of time required to run *all* configurations for the benchmark on *all available cores*.
- Run `jupyter nbconvert --execute main-figures.ipynb` and then type `ls all-figures/ | wc -l`. The reported number should be 13.
- **TODO: Open link to Cerberus**.

[multiple-cores]: https://askubuntu.com/questions/365615/how-do-i-enable-multiple-cores-in-my-virtual-enviroment
[virtualbox]: https://www.virtualbox.org


### Step-by-Step Guide

For artifact evaluation, we would like reviewers to go through the following
steps (each of which is described in detail in a section below):

- **Configurations accepted by Dahlia:** Measure how many points in a large design space are well-typed according to Dahlia's type system.
    - Exhaustive DSE: % of configurations accepted by Dahlia.
    - Qualitative Studies: % of configurations accepted by Dahlia.
- **Experimental data and graph generation:** See how the raw data for our experiments turns into the charts you see in the paper.
    - Regenerate all graphs in the paper using the `main-figures.ipynb` script.
    - (*Optional*) Open the Jupyter notebook and read the explanation for all the experiments.
    - (*Optional*) Visually inspect the collected data in the repository.
- **Data collection example:** Actually run the HLS experiments in the paper to collect the aforementioned raw data.
    - Try out the scaled down data collection example with [Polyphemus][], our server for running FPGA compilation jobs.
    - (*Optional*) Read the documentation on setting up Polyphemus on AWS. **TODO: Missing**.
    - (*Optional*) Read the documentation on setting up a new experiment with Polyphemus.
- (*Optional*) **Using the Dahlia compiler:** Compile our example programs and write your own programs, observing the output HLS C code and the error messages.
    - (*Optional*) Rebuild the compiler.
    - (*Optional*) Run the examples and check the error messages generated by the compiler.
    - (*Optional*) Check out the documentation on the language. **TODO: Requires cleanup**
    - (*Optional*) Check out the documentation on extending the compiler. **TODO: Missing**


#### Configurations accepted by Dahlia

**Estimated time: 9-10 hours on a single core machine**

We recommend reviewers use as many physical cores as they have available to speed
up this section.

In this section, we will reproduce the following claims:

*Section 5.2*
> Dahlia accepts 354 configurations, or about 1.1% of the unrestricted design space.

*Section 5.3 (stencil2d)*
> The resulting design space has 2,916 points. Dahlia accepts 18 of these points (0.6%).

*Section 5.3 (md-knn)*
> The full space has 16,384 points, of which Dahlia accepts 525 (3%).

*Section 5.3 (md-grid)*
> The full space has 21,952 points, of which Dahlia accepts 81 (0.4%)

Each claim has two parts: (1) the number of configurations in the design space, and
(2) the number of configurations accepted by Dahlia (i.e., they are well-typed according to Dahlia's type checker).

For each benchmark, our script generates *k* directories where *k* is the number
of configurations. It then runs the Dahlia compiler on each of the files and
reports if the configuration was accepted or not.

The script uses [GNU `parallel`][parallel] to speed up execution. Actual runtime will depend
on the number of cores available.

Run the following command:
```
./_scripts/run-dahlia-accepts.sh
```

The script will report number of configurations accepted for each benchmark.

[parallel]: https://www.gnu.org/software/parallel/

#### Figures and Pareto points

**Estimated time: 10-15 minutes**

In this section, we reproduce all the graphs in the paper **from data already
committed to the repository**. Because actually running the experiments and collecting the data requires access
to proprietary compilers and/or hardware, we address data collection in the next section.

- In the `dahlia-evaluation/` directory, run `jupyter notebook`. Your browser should open.
- Click on `main-figures.ipynb`.
- Click on the "Restart the kernel and re-run the whole notebook" button (⏩️).
- All the graphs will be generated within the notebook under the corresponding
  section.

**Note:** The color and the background on the graphs might look different but
the points and the labels are correct.

<details>
<summary><b>Information on saved data:</b> [click to expand]
We optionally invite the reviewers to look at our collected data. This section
describe where all the saved data is.
</summary>

**Sensitivity analysis** (`sensitivity-analysis/`)

The sensitivity analysis consists of three experiments:

1. Fig. 4a: Unrolling the innermost loop without any partitioning (`sensitivity-analysis/no-partition-unoll/summary.csv`).
2. Fig. 4b: Unrolling with a constant partitioning (`sensitivity-analysis/const-partition-unroll/summary.csv`)
3. Fig. 4c: Unrolling and partitioning in lockstep (`sensitivity-analysis/lockstep-partition-and-unroll/summary.csv`).

**Exhaustive DSE** (`exhaustive-dse/data/`)

The exhaustive design space exploration study uses a single experiment with
32,000 distinct configurations to generate the three subgraphs in Figure 7.

**Qualitative study** (`qualitative-study/data/`)

The qualitative study consists of three benchmarks:

1. stencil2d (`qualitative-study/stencil2d`).
2. md-knn (`qualitative-study/md-knn`).
3. md-grid (`qualitative-study/md-grid`).

**Spatial** (`spatial-sweep/data/`)

The Spatial study consists of one experiment with several configurations to
generate Figure 9 (main paper) and Figure 2 (supplementary text).
</details>

#### Data Collection

This section describes how to actually run the experiments to generate the raw data that goes into the plots demonstrated above.
This step is the trickiest because it requires access to proprietary [Xilinx][] toolchains and, in some cases, actual FPGA hardware.
We have attempted to make this as painless as possible by using [AWS EC2][ec2], provides a [license-free way][fpga-dev-image] to use the Xilinx toolchain and ["F1" instances][f1] that come equipped with Xilinx FPGAs,
and [Polyphemus][], a server we developed to manage large numbers of FPGA compilation and execution jobs.

Each figure in the paper requires data from different sources:

1. Sensitivity analysis (fig. 4): This benchmark synthesizes
   the hardware design in AWS F1 and runs it on an FPGA. It requires access
   to both Xilinx's Vivado HLS tools and an actual FPGA.

2. Exhaustive DSE (fig. 7) & Qualitative Studies (fig. 8): These benchmarks
   run estimation on the FPGA designs without any synthesis. They only require
   the Vivado HLS tools.

3. Spatial Comparison (fig. 8): This experiment generates post-synthesis
   resource summaries for several parameter configurations with [Spatial][]
   designs. For our submission, we were only able to get the Spatial workflow
   running on our research server (non-AWS).

[xilinx]: https://www.xilinx.com
[ec2]: https://aws.amazon.com/ec2/
[fpga-dev-image]: https://aws.amazon.com/marketplace/pp/Xilinx-Vivado-SDx-20182-Developer-AMI/B07D6Z7P37
[f1]: https://aws.amazon.com/ec2/instance-types/f1/
[spatial]: https://spatial-lang.org

These directions will not reproduce the *full* set of data reported in the paper, which is generally not practical within the evaluation time (fig. 7, for example, took us 2,666 CPU-hours to produce).
We instead provide smaller versions of each experiment that are practical to run within a reasonable amount of time.
The idea is to to demonstrate that our distributed FPGA experimentation framework is functional to give evidence that our reported data is correct.
We also provide instructions to reproduce our original results.

Each experiment goes through the following flow:

<p align="center">
  <img src="./static/data-collection.svg">
</p>

<details>
<summary><b>Example Configuration</b> [click to expand]</summary>

`gen_dse.py` is a search and replace script that generates folders for each
possible configuration.

When invoked on a folder, it looks for a `template.json` file that maps
paramters in files to possible values. For example, the following in
files in a folder named `bench`:

<table>
<tr> <th> bench.cpp </th> <th> template.json </th> </tr>
<tr>
<td>
<pre>
int x = ::CONST1::;
int y = ::CONST2::;
x + y;
</pre>
</td>
<td>
<pre>
{
  "bench.cpp": {
    "CONST1": [1, 2, 3],
    "CONST2": [1, 2, 3]
  }
}
</pre>
</td>
</tr>
</table>

`gen_dse.py` will generate 9 configurations in total by iterating over the
possible values of `CONST1` and `CONST2`.
</details>


----------------


### Benchmarking Scripts

The infrastructure for running benchmarks is under the `_scripts` directory.

For these scripts, you can set a `BUILDBOT` environment variable to point to
the URL of the running Buildbot instance.

<details>
<summary><b>batch.py</b> [click to expand]</summary>
Submit a batch of benchmark jobs to the Buildbot.

Each argument to the script should be the path to a specific benchmark version in this repository, like `baseline/machsuite-gemm-ncubed`.
Use it like this:

    ./_scripts/batch.py <benchpath1> <benchpath2> ...

The script creates a new directory for the batch under `_results/` named with a timestamp.
It puts a list of job IDs in a file called `jobs.txt` there.
It prints the name of the batch directory (i.e., the timestamp) to stdout.

This script has command-line options:

- `-E`: Submit jobs for full synthesis. (The default is to just do estimation.)
- `-p`: Pretend to submit jobs, but don't actually submit anything. (For debugging.)
</details>

<details>
<summary><b>extract.py</b> [click to expand]</summary>
Download results for a previously-submitted batch of benchmark jobs.

On the command line, give the path to the batch directory.
Like this:

    ./_scripts/extract.py _results/2019-07-13-17-13-09

The script downloads information about jobs from `jobs.txt` in that directory.
It saves lots of extracted result values for the batch in a file called `results.json` there.

</details>

<details>
<summary><b>summarize.py</b> [click to expand]</summary>
Given some extracted data for a batch, summarize the results in a human-friendly CSV.

Give the script the path to a `results.json`, like this:

    ./_scripts/summarize.py _results/2019-07-13-17-13-09/results.json

The script produces a file in the same directory called `summary.csv` with particularly relevant information pulled out.
</details>

<details>
<summary><b>status.py</b> [click to expand]</summary>
Get the current status of a batch while you impatiently wait for jobs to complete.
Print out the number of jobs in each state.

Give the script the path to a batch directory:

    ./_scripts/status.py _results/2019-07-13-17-13-09

Use the [watch](https://linux.die.net/man/1/watch) command to repeatedly run
the command every 5 seconds

    watch -n5 ./_scripts/status.py _results/2019-07-13-17-13-09
</details>

### Contact

Please open an issue or email [Rachit Nigam](mailto:rnigam@cs.cornell.edu).
