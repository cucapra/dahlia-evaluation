## Dahlia Evaluation

Evaluation for "[Predictable Accelerator Design with Time-Sensitive Affine types][dahlia-paper]"
using the [Dahlia programming language][dahlia].

There are three components to the Evaluation:

- [The Dahlia Compiler][dahlia]: A compiler from Dahlia to Vivado HLS C.
- [Polyphemus Server][poly]: A server-client system for orchestrating large scale FPGA experiments.
- Benchmarks (this repository).

[dahlia]: https://github.com/cucapra/dahlia
[dahlia-paper]: https://rachitnigam.com/files/pubs/dahlia.pdf
[poly]: https://github.com/cucapra/polyphemus/

### Prerequisites

1. Python 3
2. Install JupyterLab with `pip install jupyter`.
3. Install python dependencies with `pip install -r requirements.txt`
4. Install local benchmarking helpers with `cd benchmarking-helpers && pip install -e .`

### Evaluation Checklist

For artifact evaluation, we would like reviewers to go through the following
steps:

- [ ] The Dahlia Compiler
  - [ ] Rebuild the compiler.
  - [ ] Run the examples and check the error messages generated by the compiler.
  - [ ] (Optional) Checkout the documentation on the language. **TODO: Requires cleanup**
  - [ ] (Optional) Checkout the documentation on extending the compiler. **TODO: Missing**

- [ ] Experimental data and graph generation
  - [ ] Regenerate all graphs in the paper using the `main-figures.ipynb` script.
  - [ ] (Optional) Open the ipython notebook and read the explanation for all the experiments.
  - [ ] (Optional) Visually inspect the collected data in the repository.

- [ ] Data Collection example
  - [ ] Try out the scaled down data collection example with Polyphemus.
  - [ ] (Optional) Read the documentation on setting up Polyphemus on AWS. **TODO: Missing**.
  - [ ] (Optional) Read the documentation on setting up a new experiment with Polyphemus.

### Experiments and Case studies

The paper contains four graphs:

1. Figure 4: Sensitivity analysis of unrolling and partitioning.
2. Figure 7: Exhaustive design space exploration for gemm-blocked.
3. Figure 8: Qualitative study of MachSuite benchmarks.
4. Figure 9: Resource Utilization for gemm-ncubed in [Spatial](https://spatial-lang.org).

The following directories correspond to these experiments.

**Sensitivity analysis** (`sensitivity-analysis/`)

The sensitivity analysis consists of three experiments:

1. Fig. 4a: Unrolling the innermost loop without any partitioning (`sensitivity-analysis/no-partition-unoll`).
2. Fig. 4b: Unrolling with a constant partitioning (`sensitivity-analysis/const-partition-unroll`)
3. Fig. 4c: Unrolling and partitioning in lockstep (`sensitivity-analysis/lockstep-partition-and-unroll`).

**Exhaustive DSE** (`exhaustive-dse/`)

The exhaustive design space exploration study uses a single experiment with
32,000 distinct configurations to generate the three subgraphs in Figure 7.

**Qualitative study** (`qualitative-study/`)

The qualitative study consists of three benchmarks:

1. stencil2d (`qualitative-study/stencil2d`).
2. md-knn (`qualitative-study/md-knn`).
3. md-grid (`qualitative-study/md-grid`).

**Spatial** (`spatial-sweep/`)

The Spatial study consists of one experiment with several configurations to
generate Figure 9 (main paper) and Figure 2 (supplementary text).

### Data Collection

Each experiment goes through the following flow:

<p align="center">
  <img src="./static/data-collection.svg">
</p>

<details>
<summary><b>Example Configuration</b> [click to expand]</summary>

`gen_dse.py` is a search and replace script that generates folders for each
possible configuration.

When invoked on a folder, it looks for a `template.json` file that maps
paramters in files to possible values. For example, the following in
files in a folder named `bench`:

<table>
<tr> <th> bench.cpp </th> <th> template.json </th> </tr>
<tr>
<td>
<pre>
int x = ::CONST1::;
int y = ::CONST2::;
x + y;
</pre>
</td>
<td>
<pre>
{
  "bench.cpp": {
    "CONST1": [1, 2, 3],
    "CONST2": [1, 2, 3]
  }
}
</pre>
</td>
</tr>
</table>

`gen_dse.py` will generate 9 configurations in total by iterating over the
possible values of `CONST1` and `CONST2`.
</details>


----------------


### Benchmarking Scripts

The infrastructure for running benchmarks is under the `_scripts` directory.

For these scripts, you can set a `BUILDBOT` environment variable to point to
the URL of the running Buildbot instance.

<details>
<summary><b>batch.py</b> [click to expand]</summary>
Submit a batch of benchmark jobs to the Buildbot.

Each argument to the script should be the path to a specific benchmark version in this repository, like `baseline/machsuite-gemm-ncubed`.
Use it like this:

    ./_scripts/batch.py <benchpath1> <benchpath2> ...

The script creates a new directory for the batch under `_results/` named with a timestamp.
It puts a list of job IDs in a file called `jobs.txt` there.
It prints the name of the batch directory (i.e., the timestamp) to stdout.

This script has command-line options:

- `-E`: Submit jobs for full synthesis. (The default is to just do estimation.)
- `-p`: Pretend to submit jobs, but don't actually submit anything. (For debugging.)
</details>

<details>
<summary><b>extract.py</b> [click to expand]</summary>
Download results for a previously-submitted batch of benchmark jobs.

On the command line, give the path to the batch directory.
Like this:

    ./_scripts/extract.py _results/2019-07-13-17-13-09

The script downloads information about jobs from `jobs.txt` in that directory.
It saves lots of extracted result values for the batch in a file called `results.json` there.

</details>

<details>
<summary><b>summarize.py</b> [click to expand]</summary>
Given some extracted data for a batch, summarize the results in a human-friendly CSV.

Give the script the path to a `results.json`, like this:

    ./_scripts/summarize.py _results/2019-07-13-17-13-09/results.json

The script produces a file in the same directory called `summary.csv` with particularly relevant information pulled out.
</details>

<details>
<summary><b>status.py</b> [click to expand]</summary>
Get the current status of a batch while you impatiently wait for jobs to complete.
Print out the number of jobs in each state.

Give the script the path to a batch directory:

    ./_scripts/status.py _results/2019-07-13-17-13-09

Use the [watch](https://linux.die.net/man/1/watch) command to repeatedly run
the command every 5 seconds

    watch -n5 ./_scripts/status.py _results/2019-07-13-17-13-09
</details>

### Contact

Please open an issue or email [Rachit Nigam](mailto:rnigam@cs.cornell.edu).
